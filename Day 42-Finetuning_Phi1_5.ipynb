{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88796d28c9444cae8634138228a24c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2170d915218644cda143dc3501b71121",
              "IPY_MODEL_f1c850d38505467eb3857ea7ee8d6cda",
              "IPY_MODEL_7200b321fde04f9d81cc8c5413928166",
              "IPY_MODEL_3310578bdc70450cbc3dc5de9f8a7f0e",
              "IPY_MODEL_a944c4f6ce3b42c793cc3273d54200a9"
            ],
            "layout": "IPY_MODEL_26dd996bed454324aee1abcc6e02bbab"
          }
        },
        "2170d915218644cda143dc3501b71121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_926529954d1a478a8241faf66d8ed25b",
            "placeholder": "​",
            "style": "IPY_MODEL_46488b3f7b3b4422900bd057a434aa25",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "f1c850d38505467eb3857ea7ee8d6cda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_066f04c6603546d8a6cc869c0d168e8d",
            "placeholder": "​",
            "style": "IPY_MODEL_631ae03076a2446e91d7421c18251135",
            "value": "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"
          }
        },
        "7200b321fde04f9d81cc8c5413928166": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_c10b86422742475b80eca0ec55dcbb82",
            "style": "IPY_MODEL_0a0904004a864d848dbf6b40c470fca9",
            "value": true
          }
        },
        "3310578bdc70450cbc3dc5de9f8a7f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_12a8fcfe37954118b13c9cd84babc6c0",
            "style": "IPY_MODEL_a68eacf7088a4c19af78cf949e417a01",
            "tooltip": ""
          }
        },
        "a944c4f6ce3b42c793cc3273d54200a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7682712a5e5a423f8fbe2343ab80868c",
            "placeholder": "​",
            "style": "IPY_MODEL_68ad06d10d9a4a4dbbe2fb066894c7f7",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "26dd996bed454324aee1abcc6e02bbab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "926529954d1a478a8241faf66d8ed25b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46488b3f7b3b4422900bd057a434aa25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "066f04c6603546d8a6cc869c0d168e8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "631ae03076a2446e91d7421c18251135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c10b86422742475b80eca0ec55dcbb82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a0904004a864d848dbf6b40c470fca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12a8fcfe37954118b13c9cd84babc6c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a68eacf7088a4c19af78cf949e417a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "7682712a5e5a423f8fbe2343ab80868c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68ad06d10d9a4a4dbbe2fb066894c7f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4122e7ca0fbf4ad3bfd73add44fd5916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b7da05741594458ac617f9d00e74945",
              "IPY_MODEL_990b0ed1eecc462c8720e4196aa37488",
              "IPY_MODEL_40c7ef0d2ff8440cac8f018fe0b07a84"
            ],
            "layout": "IPY_MODEL_f7fb143b5d1d43c09538dfad9a02cfbe"
          }
        },
        "0b7da05741594458ac617f9d00e74945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cac45c04faf454d88652359384830f7",
            "placeholder": "​",
            "style": "IPY_MODEL_a6bc33f817d44f30a20b745d45d05da6",
            "value": "Tokenizing data: 100%"
          }
        },
        "990b0ed1eecc462c8720e4196aa37488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7de7e646bf5d4e10aa933daff264115d",
            "max": 7473,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_341d633d4fc04974a6765d87b84aa998",
            "value": 7473
          }
        },
        "40c7ef0d2ff8440cac8f018fe0b07a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a887a175a21048e3b04e52e297d4eaac",
            "placeholder": "​",
            "style": "IPY_MODEL_0bc20a22a0c6472cbf7267a76ddfd0f8",
            "value": " 7473/7473 [00:03&lt;00:00, 2335.40 examples/s]"
          }
        },
        "f7fb143b5d1d43c09538dfad9a02cfbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cac45c04faf454d88652359384830f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6bc33f817d44f30a20b745d45d05da6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7de7e646bf5d4e10aa933daff264115d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "341d633d4fc04974a6765d87b84aa998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a887a175a21048e3b04e52e297d4eaac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bc20a22a0c6472cbf7267a76ddfd0f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c952a32656b46aaae405d1b8cae999b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20e11a2c4b084f098ee3c49ecded8aee",
              "IPY_MODEL_f7a89feec13f4752aca15f2b801c9ef2",
              "IPY_MODEL_6f4e3ee6a8354f5e96cdea78c3bc7ab1"
            ],
            "layout": "IPY_MODEL_807b7a651b574e6481ee09bff82ff7b1"
          }
        },
        "20e11a2c4b084f098ee3c49ecded8aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bdd6e09baa7490d9e3e0b2103c00d4b",
            "placeholder": "​",
            "style": "IPY_MODEL_95cbcad94a2d4d3fa7e723b1ba2f2ef0",
            "value": "Downloading (…)/adapter_config.json: 100%"
          }
        },
        "f7a89feec13f4752aca15f2b801c9ef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9086752b364444d9047d1b6b711a072",
            "max": 440,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7760c170ba9747b095e557db408a6556",
            "value": 440
          }
        },
        "6f4e3ee6a8354f5e96cdea78c3bc7ab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e89cb9dcc1e44b14b2cbed0147e2099e",
            "placeholder": "​",
            "style": "IPY_MODEL_aa711c775bc047f898f1ca12f2bed32c",
            "value": " 440/440 [00:00&lt;00:00, 26.0kB/s]"
          }
        },
        "807b7a651b574e6481ee09bff82ff7b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bdd6e09baa7490d9e3e0b2103c00d4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95cbcad94a2d4d3fa7e723b1ba2f2ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9086752b364444d9047d1b6b711a072": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7760c170ba9747b095e557db408a6556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e89cb9dcc1e44b14b2cbed0147e2099e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa711c775bc047f898f1ca12f2bed32c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cddfa61905e74f04a3d8621ac496360a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20807a4b1fc44d2cb7e749a40cbd67b9",
              "IPY_MODEL_97cb31b2f30d4f9fbb2b1118ad25b4db",
              "IPY_MODEL_a215e87b044440a4a8060132249416ea"
            ],
            "layout": "IPY_MODEL_7332c5ae8f8d430f819211be721fc114"
          }
        },
        "20807a4b1fc44d2cb7e749a40cbd67b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0c9e3e7b95d43f48dfca5eeb80db655",
            "placeholder": "​",
            "style": "IPY_MODEL_4de74eced543498f8b82819c7a1fe768",
            "value": "Downloading adapter_model.bin: 100%"
          }
        },
        "97cb31b2f30d4f9fbb2b1118ad25b4db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12bd7286873746e4b13be6d07096f02a",
            "max": 18907665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d41052eb4ae341a4b32a51fafce1a8bb",
            "value": 18907665
          }
        },
        "a215e87b044440a4a8060132249416ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec58278d7c1940b0b4d7275b30578524",
            "placeholder": "​",
            "style": "IPY_MODEL_345aebc0a162416b8d3d7740d53380fc",
            "value": " 18.9M/18.9M [00:03&lt;00:00, 6.17MB/s]"
          }
        },
        "7332c5ae8f8d430f819211be721fc114": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c9e3e7b95d43f48dfca5eeb80db655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4de74eced543498f8b82819c7a1fe768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12bd7286873746e4b13be6d07096f02a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d41052eb4ae341a4b32a51fafce1a8bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec58278d7c1940b0b4d7275b30578524": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "345aebc0a162416b8d3d7740d53380fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05eb667848504f99b9079977ebc3e745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d287e5c1cad74622918243cac3d3a488",
              "IPY_MODEL_79b7e1c1700b43aa876477855b36ad13",
              "IPY_MODEL_073fe5fbb1f542359fdc4dc9d23126dd"
            ],
            "layout": "IPY_MODEL_d4a1d522a1e74d258e3a2007ddbb9d6a"
          }
        },
        "d287e5c1cad74622918243cac3d3a488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_796fa87f078b45eda71eea57fe702291",
            "placeholder": "​",
            "style": "IPY_MODEL_0e3e52096a6f463da5e1b0ee07f4561b",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "79b7e1c1700b43aa876477855b36ad13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e24620cd8be6456cab1044b8507aa8de",
            "max": 5673167489,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4137524f8a5448c489c028c1b7f6da3b",
            "value": 5673167489
          }
        },
        "073fe5fbb1f542359fdc4dc9d23126dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d100524511841ff8647335e253c68a8",
            "placeholder": "​",
            "style": "IPY_MODEL_7197311aa49b4bf787a8d734c80876fc",
            "value": " 5.67G/5.67G [09:03&lt;00:00, 11.3MB/s]"
          }
        },
        "d4a1d522a1e74d258e3a2007ddbb9d6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "796fa87f078b45eda71eea57fe702291": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e3e52096a6f463da5e1b0ee07f4561b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e24620cd8be6456cab1044b8507aa8de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4137524f8a5448c489c028c1b7f6da3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d100524511841ff8647335e253c68a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7197311aa49b4bf787a8d734c80876fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sam-Joshua-S/100DaysOfData/blob/main/Day%2042-Finetuning_Phi1_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Dependencies"
      ],
      "metadata": {
        "id": "2tU7z7tyJO_j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-jiAReD2env"
      },
      "outputs": [],
      "source": [
        "! pip install -q accelerate transformers einops datasets peft bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "88796d28c9444cae8634138228a24c90",
            "2170d915218644cda143dc3501b71121",
            "f1c850d38505467eb3857ea7ee8d6cda",
            "7200b321fde04f9d81cc8c5413928166",
            "3310578bdc70450cbc3dc5de9f8a7f0e",
            "a944c4f6ce3b42c793cc3273d54200a9",
            "26dd996bed454324aee1abcc6e02bbab",
            "926529954d1a478a8241faf66d8ed25b",
            "46488b3f7b3b4422900bd057a434aa25",
            "066f04c6603546d8a6cc869c0d168e8d",
            "631ae03076a2446e91d7421c18251135",
            "c10b86422742475b80eca0ec55dcbb82",
            "0a0904004a864d848dbf6b40c470fca9",
            "12a8fcfe37954118b13c9cd84babc6c0",
            "a68eacf7088a4c19af78cf949e417a01",
            "7682712a5e5a423f8fbe2343ab80868c",
            "68ad06d10d9a4a4dbbe2fb066894c7f7"
          ]
        },
        "id": "DdZRaqEg2x5K",
        "outputId": "ba23f5b6-f2c7-4458-9ecc-93b09d576b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88796d28c9444cae8634138228a24c90"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Dependencies"
      ],
      "metadata": {
        "id": "v2yHO2paJSTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import os"
      ],
      "metadata": {
        "id": "X2pEaQ0w8188"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning"
      ],
      "metadata": {
        "id": "jzzMnvP3JVyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "2x0EnK1X9L9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/phi-1_5\",\n",
        "    device_map={\"\":0},\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config\n",
        ")"
      ],
      "metadata": {
        "id": "q29CmvRzA996"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M75zCiGwBM1q",
        "outputId": "bab29094-496d-4a83-e13f-98d136858cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MixFormerSequentialForCausalLM(\n",
              "  (layers): Sequential(\n",
              "    (0): Embedding(\n",
              "      (wte): Embedding(51200, 2048)\n",
              "      (drop): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (2): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (3): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (4): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (5): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (6): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (7): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (8): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (9): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (10): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (11): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (12): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (13): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (14): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (15): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (16): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (17): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (18): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (19): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (20): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (21): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (22): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (23): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (24): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (25): CausalLMHead(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (linear): Linear(in_features=2048, out_features=51200, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (loss): CausalLMLoss(\n",
              "    (loss_fct): CrossEntropyLoss()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"Wqkv\", \"out_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVSrZLrUBegg",
        "outputId": "66941fb3-7b05-44ae-d73b-8b8f9d6f3579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,718,592 || all params: 1,422,989,312 || trainable%: 0.3315971497613047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "QYNq0WmSKQ0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65603a96-4b81-430f-814a-d0544764ba85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): MixFormerSequentialForCausalLM(\n",
            "      (layers): Sequential(\n",
            "        (0): Embedding(\n",
            "          (wte): Embedding(51200, 2048)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (1): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (2): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (3): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (4): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (5): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (6): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (7): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (8): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (9): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (10): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (11): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (12): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (13): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (14): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (15): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (16): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (17): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (18): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (19): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (20): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (21): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (22): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (23): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (24): ParallelBlock(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (mixer): MHA(\n",
            "            (rotary_emb): RotaryEmbedding()\n",
            "            (Wqkv): Linear4bit(\n",
            "              in_features=2048, out_features=6144, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=6144, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (out_proj): Linear4bit(\n",
            "              in_features=2048, out_features=2048, bias=True\n",
            "              (lora_dropout): ModuleDict(\n",
            "                (default): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (lora_A): ModuleDict(\n",
            "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "              )\n",
            "              (lora_B): ModuleDict(\n",
            "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "              )\n",
            "              (lora_embedding_A): ParameterDict()\n",
            "              (lora_embedding_B): ParameterDict()\n",
            "            )\n",
            "            (inner_attn): SelfAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (inner_cross_attn): CrossAttention(\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (mlp): MLP(\n",
            "            (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
            "            (act): NewGELUActivation()\n",
            "          )\n",
            "        )\n",
            "        (25): CausalLMHead(\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (linear): Linear(in_features=2048, out_features=51200, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (loss): CausalLMLoss(\n",
            "        (loss_fct): CrossEntropyLoss()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sample):\n",
        "    model_inps =  tokenizer(sample[\"text\"], padding=True, truncation=True, max_length=512)\n",
        "    return model_inps"
      ],
      "metadata": {
        "id": "mnbKMOIO9pWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
        "data_df = data.to_pandas()\n",
        "data_df[\"text\"] = data_df[[\"question\", \"answer\"]].apply(lambda x: \"question: \" + x[\"question\"] + \" answer: \" + x[\"answer\"], axis=1)\n",
        "data = Dataset.from_pandas(data_df)\n",
        "tokenized_data = data.map(tokenize, batched=True, desc=\"Tokenizing data\", remove_columns=data.column_names)\n",
        "tokenized_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146,
          "referenced_widgets": [
            "4122e7ca0fbf4ad3bfd73add44fd5916",
            "0b7da05741594458ac617f9d00e74945",
            "990b0ed1eecc462c8720e4196aa37488",
            "40c7ef0d2ff8440cac8f018fe0b07a84",
            "f7fb143b5d1d43c09538dfad9a02cfbe",
            "9cac45c04faf454d88652359384830f7",
            "a6bc33f817d44f30a20b745d45d05da6",
            "7de7e646bf5d4e10aa933daff264115d",
            "341d633d4fc04974a6765d87b84aa998",
            "a887a175a21048e3b04e52e297d4eaac",
            "0bc20a22a0c6472cbf7267a76ddfd0f8"
          ]
        },
        "id": "fjszkbOx9YnY",
        "outputId": "f3c6dfb3-4f19-484d-89b2-691f317f8200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing data:   0%|          | 0/7473 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4122e7ca0fbf4ad3bfd73add44fd5916"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask'],\n",
              "    num_rows: 7473\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "        output_dir=\"phi-1_5-finetuned-gsm8k\",\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=1,\n",
        "        learning_rate=2e-4,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_steps=100,\n",
        "        max_steps=1000,\n",
        "        num_train_epochs=1,\n",
        "        push_to_hub=True\n",
        "    )"
      ],
      "metadata": {
        "id": "5ZVmWzSn-BO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_data,\n",
        "    args=training_arguments,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "trainer.train()\n",
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "hzggVLwo24vi",
        "outputId": "22cfea4f-e68b-4c86-d719-c7dbfa252cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a CodeGenTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 19:27, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.162600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.054700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.014700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.053000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.017000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.033500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.012000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.027300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.012200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.029100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://huggingface.co/SamJoshua/phi-1_5-finetuned-gsm8k/tree/main/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving"
      ],
      "metadata": {
        "id": "FMSpobPBJM5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=torch.float32)\n",
        "peft_model = PeftModel.from_pretrained(model, \"SamJoshua/phi-1_5-finetuned-gsm8k\", from_transformers=True)\n",
        "model = peft_model.merge_and_unload()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1c952a32656b46aaae405d1b8cae999b",
            "20e11a2c4b084f098ee3c49ecded8aee",
            "f7a89feec13f4752aca15f2b801c9ef2",
            "6f4e3ee6a8354f5e96cdea78c3bc7ab1",
            "807b7a651b574e6481ee09bff82ff7b1",
            "8bdd6e09baa7490d9e3e0b2103c00d4b",
            "95cbcad94a2d4d3fa7e723b1ba2f2ef0",
            "e9086752b364444d9047d1b6b711a072",
            "7760c170ba9747b095e557db408a6556",
            "e89cb9dcc1e44b14b2cbed0147e2099e",
            "aa711c775bc047f898f1ca12f2bed32c",
            "cddfa61905e74f04a3d8621ac496360a",
            "20807a4b1fc44d2cb7e749a40cbd67b9",
            "97cb31b2f30d4f9fbb2b1118ad25b4db",
            "a215e87b044440a4a8060132249416ea",
            "7332c5ae8f8d430f819211be721fc114",
            "c0c9e3e7b95d43f48dfca5eeb80db655",
            "4de74eced543498f8b82819c7a1fe768",
            "12bd7286873746e4b13be6d07096f02a",
            "d41052eb4ae341a4b32a51fafce1a8bb",
            "ec58278d7c1940b0b4d7275b30578524",
            "345aebc0a162416b8d3d7740d53380fc"
          ]
        },
        "id": "gztQI9_vH06k",
        "outputId": "f202b367-ee89-474c-f603-66b49e6aa06a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/adapter_config.json:   0%|          | 0.00/440 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c952a32656b46aaae405d1b8cae999b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading adapter_model.bin:   0%|          | 0.00/18.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cddfa61905e74f04a3d8621ac496360a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MixFormerSequentialForCausalLM(\n",
              "  (layers): Sequential(\n",
              "    (0): Embedding(\n",
              "      (wte): Embedding(51200, 2048)\n",
              "      (drop): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (2): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (3): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (4): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (5): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (6): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (7): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (8): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (9): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (10): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (11): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (12): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (13): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (14): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (15): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (16): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (17): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (18): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (19): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (20): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (21): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (22): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (23): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (24): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (25): CausalLMHead(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (linear): Linear(in_features=2048, out_features=51200, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (loss): CausalLMLoss(\n",
              "    (loss_fct): CrossEntropyLoss()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"SamJoshua/phi-1_5-finetuned-gsm8k\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164,
          "referenced_widgets": [
            "05eb667848504f99b9079977ebc3e745",
            "d287e5c1cad74622918243cac3d3a488",
            "79b7e1c1700b43aa876477855b36ad13",
            "073fe5fbb1f542359fdc4dc9d23126dd",
            "d4a1d522a1e74d258e3a2007ddbb9d6a",
            "796fa87f078b45eda71eea57fe702291",
            "0e3e52096a6f463da5e1b0ee07f4561b",
            "e24620cd8be6456cab1044b8507aa8de",
            "4137524f8a5448c489c028c1b7f6da3b",
            "0d100524511841ff8647335e253c68a8",
            "7197311aa49b4bf787a8d734c80876fc"
          ]
        },
        "id": "_RD3fKuGI5pY",
        "outputId": "248e9e9b-f50d-4712-da4a-cd85ff59f0a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/5.67G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05eb667848504f99b9079977ebc3e745"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/SamJoshua/phi-1_5-finetuned-gsm8k/commit/0da70a2ec9406f5f671a620cc02d90e66e3af640', commit_message='Upload MixFormerSequentialForCausalLM', commit_description='', oid='0da70a2ec9406f5f671a620cc02d90e66e3af640', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "xkYdH7rOJI7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, AutoModel\n",
        "from rich import print as rprint\n",
        "from rich.markdown import Markdown\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/phi-1_5', trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained('SamJoshua/phi-1_5-finetuned-gsm8k', trust_remote_code=True, eos_token_id=tokenizer.eos_token_id).cuda()\n",
        "\n",
        "prompt = \"\"\"question:\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\"\t answer:\"\"\"\n",
        "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
        "output_ids = model.generate(input_ids, max_new_tokens=100, do_sample=True, top_p=0.9, top_k=0, temperature=0.01, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id, repetition_penalty=1.2)\n",
        "rprint(Markdown(tokenizer.decode(output_ids[0], skip_special_tokens=True)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "zZ5SXmwX3_w9",
        "outputId": "1cb292c5-ab57-46ef-c3ab-8a2fea7aca5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "question:\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did  \n",
              "she earn?\"     answer: She worked 0 hours and 30 mins because 1/2 = <<1*60+30=90>>90 She earned 90 dollars         \n",
              "yesterday so 12 * 2 / 60 + 120 - 10 <-- that's how many cents it was worth in total to her! 1200 > 100000 -> true!!\n",
              "1000 is the amount WENG got paid today!!!####1000<-true### 2000 more than last week #########2000> 3000 less then  \n",
              "this month too 💰📉 4000 times a                                                                                   \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">question:\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did  \n",
              "she earn?\"     answer: She worked 0 hours and 30 mins because 1/2 = &lt;&lt;1*60+30=90&gt;&gt;90 She earned 90 dollars         \n",
              "yesterday so 12 * 2 / 60 + 120 - 10 &lt;-- that's how many cents it was worth in total to her! 1200 &gt; 100000 -&gt; true!!\n",
              "1000 is the amount WENG got paid today!!!####1000&lt;-true### 2000 more than last week #########2000&gt; 3000 less then  \n",
              "this month too 💰📉 4000 times a                                                                                   \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CBGp8_3p4sc0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}